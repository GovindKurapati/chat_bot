{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "import requests\n",
    "import ollama\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up keys (you can load from env or file securely)\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57edcbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OPENAI FUNCTIONS\n",
    "def ask_openai(message):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4\"\n",
    "        messages=[{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def stream_openai(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    for user, bot in history:\n",
    "        messages.extend([\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "            {\"role\": \"assistant\", \"content\": bot}\n",
    "        ])\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    collected = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.get(\"content\"):\n",
    "            token = chunk.choices[0].delta[\"content\"]\n",
    "            collected += token\n",
    "            yield history + [[message, collected]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e770087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEMINI FUNCTIONS\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "def ask_gemini(message):\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "    chat = model.start_chat(history=[])\n",
    "    response = chat.send_message(message)\n",
    "    response.resolve()\n",
    "    return response.text\n",
    "\n",
    "def stream_gemini(message, history):\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "    chat = model.start_chat(history=[])\n",
    "    for user, bot in history:\n",
    "        chat.history.append({\"role\": \"user\", \"parts\": [{\"text\": user}]})\n",
    "        chat.history.append({\"role\": \"model\", \"parts\": [{\"text\": bot}]})\n",
    "\n",
    "    stream = chat.send_message(message, stream=True)\n",
    "    collected = \"\"\n",
    "    for chunk in stream:\n",
    "        collected += chunk.text\n",
    "        yield history + [[message, collected]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f371ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANTHROPIC FUNCTIONS\n",
    "def ask_anthropic(message):\n",
    "    client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=512,\n",
    "        messages=[{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def stream_anthropic(message, history):\n",
    "    messages = [{\"role\": \"user\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    for user, bot in history:\n",
    "        messages.extend([\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "            {\"role\": \"assistant\", \"content\": bot}\n",
    "        ])\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        messages=messages,\n",
    "        max_tokens=1024,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    collected = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.type == \"content_block_delta\":\n",
    "            token = chunk.delta.text\n",
    "            collected += token\n",
    "            yield history + [[message, collected]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLLAMA FUNCTIONS\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "def ask_ollama(message):\n",
    "    print(\"Asking Ollama:\", message)\n",
    "    res = requests.post(\n",
    "        \"http://localhost:11434/api/chat\",\n",
    "        json={\"model\": \"llama3.2\", \"messages\": [{\"role\": \"user\", \"content\": message}], \"headers\": HEADERS, \"stream\": False}\n",
    "    )\n",
    "    print(\"Ollama response:\", res.json())\n",
    "    print(res.json()[\"message\"][\"content\"])\n",
    "    return res.json()[\"message\"][\"content\"]\n",
    "\n",
    "def stream_ollama(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    for user, assistant in history:\n",
    "        messages += [{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}]\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # response = requests.post(\n",
    "    #     \"http://localhost:11434/api/chat\",\n",
    "    #     json={\"model\": \"llama3.2\", \"messages\": messages, \"stream\": True},\n",
    "    #     stream=True,\n",
    "    # )\n",
    "\n",
    "    print(\"Streaming Ollama:\", messages)\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.2\",\n",
    "        messages= messages,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    collected = \"\"\n",
    "    for chunk in response:\n",
    "        token = chunk[\"message\"][\"content\"]\n",
    "        collected += token\n",
    "        yield history + [[message, collected]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master response handler\n",
    "def response(message, history, model_choice):\n",
    "    try:\n",
    "        if model_choice == \"OpenAI\":\n",
    "            return ask_openai(message)\n",
    "        elif model_choice == \"Gemini\":\n",
    "            return ask_gemini(message)\n",
    "        elif model_choice == \"Anthropic\":\n",
    "            return ask_anthropic(message)\n",
    "        elif model_choice == \"Ollama\":\n",
    "            return ask_ollama(message)\n",
    "        else:\n",
    "            return \"Model not supported.\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Error: {e}\"\n",
    "    \n",
    "# for streaming responses\n",
    "def handle_message(message, history, model_choice):\n",
    "    if not message.strip():\n",
    "        yield history + [[\"-\", \"Please enter a message\"]], history\n",
    "    generators = {\n",
    "        \"OpenAI\": stream_openai,\n",
    "        \"Gemini\": stream_gemini,\n",
    "        \"Anthropic\": stream_anthropic,\n",
    "        \"Ollama\": stream_ollama,\n",
    "    }\n",
    "\n",
    "    generator = generators.get(model_choice)\n",
    "    if not generator:\n",
    "        yield history + [[message, \"‚ùå Invalid model selected\"]], history\n",
    "\n",
    "    final_response = \"\"\n",
    "    for updated_history in generator(message, history):\n",
    "        final_response = updated_history[-1][1]\n",
    "        yield updated_history, updated_history  # Update both chatbot + state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Gradio Interface without streaming\n",
    "with gr.Blocks() as demo:\n",
    "    model_dropdown = gr.Dropdown(choices=[\"OpenAI\", \"Gemini\", \"Anthropic\", \"Ollama\"],\n",
    "                                  value=\"Gemini\", label=\"Select Model\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(placeholder=\"Ask a question...\")\n",
    "    send_btn = gr.Button(\"Send\")\n",
    "\n",
    "    history = gr.State([])\n",
    "\n",
    "    def chat_flow(message, chat_history, model_choice):\n",
    "        response_text = response(message, chat_history, model_choice)\n",
    "        chat_history.append((message, response_text))\n",
    "        return chat_history, chat_history\n",
    "\n",
    "    send_btn.click(chat_flow, inputs=[msg, history, model_dropdown], outputs=[chatbot, history])\n",
    "    msg.submit(chat_flow, inputs=[msg, history, model_dropdown], outputs=[chatbot, history])\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff39d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio with streaming\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ü§ñ Multi-LLM Chatbot with Streaming + Markdown\")\n",
    "    gr.Markdown(\"Select a model below and chat with Gemini, GPT, Claude, or Ollama!\")\n",
    "\n",
    "    model_selector = gr.Dropdown(\n",
    "        choices=[\"OpenAI\", \"Gemini\", \"Anthropic\", \"Ollama\"],\n",
    "        value=\"Gemini\",\n",
    "        label=\"Choose a Model\"\n",
    "    )\n",
    "\n",
    "    chatbot = gr.Chatbot(render_markdown=True, avatar_images=(\"https://cdn-icons-png.flaticon.com/512/9131/9131478.png\",\"https://cdn-icons-png.flaticon.com/512/13330/13330989.png\"))\n",
    "    msg = gr.Textbox(placeholder=\"Type your message here...\", show_label=False)\n",
    "    send = gr.Button(\"Send\", variant=\"huggingface\")\n",
    "    clear = gr.Button(\"Clear Chat\", variant=\"stop\")\n",
    "    state = gr.State([])\n",
    "    send.click(handle_message, [msg, state, model_selector], [chatbot, state], concurrency_limit=1)\n",
    "    msg.submit(handle_message, [msg, state, model_selector], [chatbot, state], concurrency_limit=1)\n",
    "\n",
    "    def reset():\n",
    "        return [], []\n",
    "\n",
    "    clear.click(reset, None, [chatbot, state])\n",
    "\n",
    "demo.launch(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
